---
title: "Econometrics III"
subtitle: "![](mnb_intezet.png){ width=30% } \n Big Data and Data Visualisation"
author: "Marcell Granát"
institute: "Central Bank of Hungary & .blue[John von Neumann University]"
date: "2022"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
  seal: false
editor_options: 
  chunk_output_type: console
---

```{css, echo=FALSE}
.red { color: red; }
.blue { color: #378C95; }
strong { color: red; }
a { color: #378C95; font-weight: bold; }
.remark-inline-code { font-weight: 900; background-color: #a7d5e7; }
.caption { color: #378C95; font-style: italic; text-align: center; }

.content-box { 
box-sizing: content-box;
background-color: #378C95;
/* Total width: 160px + (2 * 20px) + (2 * 8px) = 216px
Total height: 80px + (2 * 20px) + (2 * 8px) = 136px
Content box width: 160px
Content box height: 80px */
}

.content-box-green {
background-color: #d9edc2;
}

.content-box-red {
background-color: #f9dbdb;
}

.fullprice {
text-decoration: line-through;
}
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(knitr)
library(granatlib)
library(emo)
library(magrittr)
library(tidyverse)
library(patchwork)
style_mono_accent(
  base_color = "#DC322F",               # bright red
  inverse_background_color = "#002B36", # dark dark blue
  inverse_header_color = "#378C95",     # light aqua green
  inverse_text_color = "#FFFFFF",       # white
  title_slide_background_color = "var(--base)",
  text_font_google = google_font("Kelly Slab"),
  header_font_google = google_font("Oleo Script")
)

xaringanExtra::use_panelset()
xaringanExtra::html_dependency_clipboard()
xaringanExtra::html_dependency_scribble(pen_color = "#378C95", 3, 4)
xaringanExtra::use_tile_view()

xaringanExtra::style_panelset_tabs(
  active_foreground = "#378C95"
)
```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE,
                      fig.align = "center", 
                      error = TRUE,
                      message = F,
                      out.width = "700px",
                      fig.width = 7,
                      fig.height = 4.5, 
                      out.height = "450px",
                      dpi = 400,
                      warning = FALSE)
```

# Today's .blue[Agenda]

### Logit

---

class: inverse, middle, center

# Logit

---

## Motivation

- It has already been discussed how to incorporate categorical variables as regressors into the model

- Today we will talk about what to do if the outcome variable is categorical (For simplicity, we only look at cases where **two outcomes** are possible)

- An illustrative example (Hungarian): [Empirical investigation of yield curves' ability to predict recession](https://marcellgranat.github.io/yieldcurve/)

---

## Introduction

> "For a number of reasons, the government bond yield curve is proving to be an accurate predictor of recessions in the US"

<p align="center"><iframe width="720" height="405" src="https://www.youtube.com/embed/oW4hfaiXKG8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>

---

## Setup - Recession data

NBER publishes officially the quarters of recessions

[https://www.nber.org/research/data/us-business-cycle-expansions-and-contractions](https://www.nber.org/research/data/us-business-cycle-expansions-and-contractions)

```{r out.height="350px", out.width="500px"}
include_graphics("econometrics3/nber.png")
```


Let's take that table!

---

## Setup - Recession data

```{r echo=TRUE, eval=FALSE}
library(rvest)

recession_usa_df <- read_html("https://www.nber.org/research/data/us-business-cycle-expansions-and-contractions") %>%
  html_table() %>%
  first() %>% 
  janitor::row_to_names(1) %>% 
  janitor::clean_names() %>% 
  transmute(
    start_date = str_c(peak_year, "-", peak_quarter),
    end_date = str_c(trough_year, "-", trough_quarter),
    start_date = ifelse(str_detect(start_date, "occurred"), str_extract(start_date, "\\d\\d\\d\\dQ\\d"), start_date),
    end_date = ifelse(str_detect(end_date, "occurred"), str_extract(end_date, "\\d\\d\\d\\dQ\\d"), end_date),
    start_date = str_replace(start_date, "-", "Q"),
    end_date = str_replace(end_date, "-", "Q"),
    start_date = lubridate::yq(start_date),
    end_date = lubridate::yq(end_date)
  ) %>% 
  drop_na() %$% 
  map2(start_date, end_date, ~ seq.Date(.x, .y, by = "quarter")) %>% 
  reduce(c) %>% 
  tibble(date = ., recession = TRUE) %>% 
  write_csv("recession_usa.csv")
```

```{r}
recession_usa_df <- read_csv("econometrics3/recession_usa.csv")
```

---

## Setup - Recession data

```{r echo=TRUE}
recession_usa_df <- recession_usa_df %>% 
  full_join(tibble(date = seq.Date(as.Date("1857-01-01"), as.Date("2021-01-01"), by = "quarter"))) %>% 
  arrange(date) %>% 
  mutate(recession = ifelse(is.na(recession), FALSE, TRUE))

recession_usa_df
```

---

## Setup - Yield data

- Yield data from the FRED website

--

Yes, it is prepared for now `r ji("laugh")`

```{r echo=TRUE, eval=FALSE}
yield_usa_df <- read_csv("https://raw.githubusercontent.com/MarcellGranat/big_data2022/main/econometrics3/yield_usa.csv", col_select = -1)
```

```{r echo=FALSE}
yield_usa_df <- read_csv("econometrics3/yield_usa.csv", col_select = -1)
```

--

But, the recession data is quarterly...

```{r}
yield_usa_df <- yield_usa_df %>% 
  mutate(
    date = lubridate::floor_date(date, "quarter")
  ) %>% 
  group_by(date) %>% 
  summarise_at(-1, mean, na.rm = T)
```



---

## Setup - Join the two sets

```{r echo=TRUE}
inner_join(
  x = yield_usa_df,
  y = recession_usa_df
) %>% 
  transmute(
    date,
    recession,
    spread = GS10 - GS1,
    spread = lag(spread, n = 4)
  ) %>% 
  drop_na()
```

## Final setup

```{r echo=TRUE, results='hide'}
recession_to_spread <- function(short = "GS1", long = "GS10", n_lag = 4) {
  inner_join(
    x = yield_usa_df,
    y = recession_usa_df
  ) %>% 
    select(date, recession, short, long) %>% 
    set_names("date", "recession", "short", "long") %>% 
    transmute(
      date,
      recession,
      spread = long - short,
      spread = lag(spread, n = n_lag)
    ) %>% 
    drop_na()
}
```


--

.blue[Now, let's see the model]

---

## The logit

Let P_i denote the probability of being `TRUE`. The estimated function is the following:

$$\text{log}\frac{P_i}{1-P_i}=\beta_0+\sum_{j=1}^{k}\beta_jx_{i,j}$$

- The left side of the equation is called **log odds-ratio**

- This model is not estimated by OLS!

--

- We must introduce the likelihood-function:

$$L=\prod_{y_i=1}P_i\prod_{y_i=0}(1-P_i)$$

.blue[How would you interpret it?]

--

> Maximization of the likelihood function for either the probit or the logit model is accomplished by .blue[iterative] nonlinear estimation methods. There are now several computer programs available for probit and logit analysis, and these programs are very inexpensive to run.

---

### The Logit in R

```{r echo=TRUE}
fit <- glm(recession ~ spread, binomial(link = "logit"), recession_to_spread())
```

--

The broom functions also work here!

Extract the coeffitients:

```{r echo=TRUE}
broom::tidy(fit)
```

---

### The Logit in R

Predictions & errors for the observations:

```{r echo=TRUE}
broom::augment(fit, type.predict = "response")
```

---

### Reproduce the table from Estrella és Fishkin [1996b]

Estrella és Fishkin [1996b] reports a table that contains the probability of recession to certain values of spread.

--

To reproduce this table we use our own model to estimate the probability of recession.footnote[The results will differ a bit. This is because the time-series of spread is longer in the article & they use probit model (the link function is different, but does not cause a huge difference)]. 

--

The `augment` function has a `newdata` input parameter, so we generate the predictions so easily with that. (This works also in the case of other linear models).

---

### Reproduce the table from Estrella és Fishkin [1996b]

```{r echo=TRUE}
recession_to_spread() %>% 
  filter(date < "1995-01-01") %>% 
  glm(recession ~ spread, binomial(link = "probit"), data = .) %>% 
  broom::augment(fit, type.predict = "response", 
                 newdata=tibble(spread = seq(
                   from = -2.5, 
                   to = 1.4, 
                   by = .40)
                   ))
```


---

### ROC curve

#### When do we say that "the winter is coming"?

If it exceeds a certain probability... = **Cut-off value**

```{r echo=TRUE}
cut_off = 0.5

broom::augment(fit, type.predict = "response") %>% 
  mutate(
    estimation = .fitted > cut_off,
  ) %>% 
  select(recession, spread, .fitted, estimation)
```


---

### ROC curve

#### When do we say that "the winter is coming"?

If it exceeds a certain probability... = **Cut-off value**

```{r echo=TRUE}
cut_off = 0.5

broom::augment(fit, type.predict = "response") %>% 
  mutate(
    estimation = .fitted > cut_off,
    correct = recession == estimation
  ) %>% 
  count(correct, estimation) %>% 
  mutate(estimation = ifelse(estimation == TRUE, "positive", "negative"))
```

---

### ROC curve

#### When do we say that "the winter is coming"?

If it exceeds a certain probability... = **Cut-off value**

```{r echo=TRUE}
confusion_matrix <- function(.fit, cut_off = .5) {
  broom::augment(fit, type.predict = "response") %>% 
    mutate(
      estimation = .fitted > cut_off,
      correct = recession == estimation
    ) %>% 
    count(correct, estimation) %>% 
    mutate(estimation = ifelse(estimation == TRUE, "positive", "negative"))
}
confusion_matrix(fit, .7)
```

---

### ROC curve

#### When do we say that "the winter is coming"?

**If it exceeds a certain probability... = Cut-off value**

Some possible cut-off values:

- .5

- return the same number of `TRUE` predictions as `TRUE` observations

- maximizing the accuracy

- minimizing cost (if you know that)

---

### ROC curve

Let's construct the confusion matrix to a given a cut-off value

Now, the following indicators can be derived:

$$\text{Sensitivity}=\frac{TP}{TP+FN}$$

$$\text{Specificity}=\frac{TN}{TN+FP}$$

This two values can be calculated to any cut-off values... This is the ROC curve

---

### ROC curve in R

```{r echo=TRUE}
roc_curve <- broom::augment(fit, type.predict = "response") %$% 
  pROC::roc(recession, .fitted)
```

Area under the curve:

```{r echo=TRUE}
roc_curve$auc
```

.content-box-greeen[
AUC is the suggested indicator to evaluate your model and compare it to another one!
]

```{r echo=TRUE}
roc_curve_df <- roc_curve %$%
  tibble(thresholds, sensitivities, specificities)
```

---

### Visualization of the ROC curve

```{r}
roc_curve_df %>% 
  ggplot(aes(1- specificities, sensitivities)) +
  geom_abline(lty = 2) +
  geom_path(color = "red2", size = 1.3)
```

---

# References

```{r, print_refs, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
library(RefManageR)
bib <- ReadBib("./econometrics.bib", check = FALSE)

ref_page <- 2
for (bib_item in seq_along(bib)) {
  
  if (bib_item %% 5 == 0) {
    cat("\n")
    cat("---") 
    cat("\n")
    cat("# References ", ref_page)
    ref_page <- ref_page + 1
    cat("\n")
  }
  
  print(bib[bib_item], 
        .opts = list(check.entries = FALSE, 
                     style = "html", 
                     bib.style = "authoryear"))
}
```

---

class: center, middle

# Thank you for your attention!

Slides are available at [www.marcellgranat.com](https://www.marcellgranat.com)


